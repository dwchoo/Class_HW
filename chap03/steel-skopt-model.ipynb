{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_min_max(loss):\n",
    "    if math.isnan(loss):\n",
    "        loss = 1e+5\n",
    "    else:\n",
    "        loss = min(float(loss), 1e+5)\n",
    "        loss = max(float(loss), 1e-10)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_steal_dataset():\n",
    "    with open('../../data/chap03/faults.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "\n",
    "    global data, input_cnt, output_cnt\n",
    "    input_cnt, output_cnt = 27, 7\n",
    "    data = np.asarray(rows, dtype='float32')\n",
    "    x = data[:,:input_cnt]\n",
    "    y = data[:,-output_cnt:]\n",
    "    return data,x,y\n",
    "    \n",
    "def kfold_data(data):\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    kf.get_n_splits(data)\n",
    "    kfold_data = {}\n",
    "    for index, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "        train_data = data[train_index]\n",
    "        test_data = data[test_index]\n",
    "        kfold_data[index] = {'train' : train_data, 'test' : test_data}\n",
    "    return kfold_data\n",
    "\n",
    "def kfold_return_train(n_fold, kfold_data):\n",
    "    output_cnt = 7\n",
    "    if n_fold not in range(10):\n",
    "        print('{} is not in range(10)'.format(n_fold))\n",
    "        raise NameError('Change n_fold')\n",
    "    train_data = kfold_data[n_fold]['train']\n",
    "    #test_data = kfold_data[n_fold]['test']\n",
    "    train_data_input = train_data[:, :-output_cnt]\n",
    "    train_data_output = train_data[:, -output_cnt:]\n",
    "    return [train_data_input, train_data_output]\n",
    "def kfold_return_test(n_fold, kfold_data):\n",
    "    output_cnt = 7\n",
    "    if n_fold not in range(10):\n",
    "        print('{} is not in range(10)'.format(n_fold))\n",
    "        raise NameError('Change n_fold')\n",
    "    #train_data = kfold_data[n_fold]['train']\n",
    "    test_data = kfold_data[n_fold]['test']\n",
    "    test_data_input = test_data[:, :-output_cnt]\n",
    "    test_data_output = test_data[:, -output_cnt:]\n",
    "    return [test_data_input, test_data_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorycal Cross-entropy  \n",
    "$L(y,\\hat{y}) = - \\sum_{j=0}^M \\sum_{i=0}^N (y_{i,j}*\\log(\\hat{y}_{i,j}))$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steel_model:\n",
    "    def __init__(self,learning_rate = 0.001,n_fold = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_shape = 27 #input_cnt\n",
    "        self.output_shape = 7 #output_cnt\n",
    "        \n",
    "        self.model = self.define_model()\n",
    "        \n",
    "        \n",
    "    def init_data(self, train_data, test_data):\n",
    "        self.X_train = train_data[0]\n",
    "        self.Y_train = train_data[1]\n",
    "        self.X_test = test_data[0]\n",
    "        self.Y_test = test_data[1]\n",
    "        \n",
    "    def define_model(self,verbose = 0):\n",
    "        x = Input(shape=(self.input_shape))\n",
    "        y = Dense(self.output_shape, activation='softmax')(x)\n",
    "        __model = Model(x, y)\n",
    "        if verbose is not 0: __model.summary()\n",
    "        return __model\n",
    "    \n",
    "    def model_compile(self):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                           metrics=['accuracy']\n",
    "                          )\n",
    "    \n",
    "    def model_fit(self,verbose = 0):\n",
    "        self.model.fit(x = self.X_train, y = self.Y_train,\n",
    "                       #validation_split=0.05,\n",
    "                       #shuffle=True,\n",
    "                       validation_data = (self.X_test, self.Y_test),\n",
    "                       batch_size = 64,\n",
    "                       epochs = 30,\n",
    "                       verbose = verbose\n",
    "                      )\n",
    "        \n",
    "    def model_evaluate(self,verbose=0):\n",
    "        output = self.model.predict(self.X_test,verbose=verbose)\n",
    "        test_loss, test_acc = self.model.evaluate(self.X_test, self.Y_test,verbose=verbose)\n",
    "        \n",
    "        if verbose is not 0:\n",
    "            print(\"acc: {:2.4f}, loss: {:2.4f}\".format(test_acc, test_loss))\n",
    "        \n",
    "        result_dict = {'acc' : test_acc, 'loss' : test_loss}\n",
    "\n",
    "        return result_dict\n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data, x,y = load_steal_dataset()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = steel_model()\n",
    "model.init_data(\n",
    "    train_data=[x_train, y_train],\n",
    "    test_data=[x_test, y_test]\n",
    ")\n",
    "model.model_compile()\n",
    "model.model_fit(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steal_model_optimize:\n",
    "    def __init__(self,learning_rate = 0.01):\n",
    "        self.input_cnt, self.output_cnt = 27, 7\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "        raw_data = self.load_steal_data()\n",
    "        self._kfold_data = self.kfold_data(raw_data)\n",
    "        \n",
    "        \n",
    "    def model_trainning(self, train_data, test_data, proc_num=None, return_dict = None):\n",
    "        model_init = steel_model(learning_rate=self.learning_rate)\n",
    "        model_init.init_data(train_data = train_data,\n",
    "                             test_data = test_data)\n",
    "        model_init.model_compile()\n",
    "        model_init.model_fit(verbose=0)\n",
    "        model_result_dict = model_init.model_evaluate(verbose=0)\n",
    "        \n",
    "        if proc_num is not None and return_dict is not None:\n",
    "            return_dict[proc_num] = model_result_dict\n",
    "\n",
    "        return model_result_dict\n",
    "        \n",
    "    def model_train_multiprocess(self,):\n",
    "        kfold_data_set = []\n",
    "        for index in range(10):\n",
    "            train_data = self.kfold_return_train(n_fold=index, kfold_data=self._kfold_data)\n",
    "            test_data = self.kfold_return_test(n_fold=index, kfold_data=self._kfold_data)\n",
    "            kfold_data_set.append((train_data, test_data))\n",
    "            \n",
    "        manager = Manager()\n",
    "        return_dict = manager.dict()\n",
    "        \n",
    "        procs = []\n",
    "        for index, (train_data, test_data) in enumerate(kfold_data_set):\n",
    "            #print(train_data[0].shape, test_data[0].shape)\n",
    "            proc = Process(target=self.model_trainning,\n",
    "                           args=(train_data, test_data,\n",
    "                                 index, return_dict),\n",
    "                           name='{}-fold'.format(index)\n",
    "                          )\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "            \n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "            \n",
    "        return return_dict\n",
    "    \n",
    "    \n",
    "    def kfold_model_evaluate_result(self,return_dict, verbose = 0):\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        losses = []\n",
    "        \n",
    "        for index in range(10):\n",
    "            acc     = return_dict[index]['acc']\n",
    "            loss    = return_dict[index]['loss']\n",
    "\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        mean_acc    = np.array(accuracies).mean()\n",
    "        mean_loss = np.array(losses).mean()\n",
    "        \n",
    "        return_result_list = [mean_acc, mean_loss]\n",
    "        #print(return_result_list)\n",
    "        if verbose is not 0:\n",
    "            print(\"acc: {:2.4f}, loss: {:2.4f}\".format(*return_result_list))\n",
    "        \n",
    "\n",
    "        return return_result_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_steal_data(self,):\n",
    "        data, x, y = load_steal_dataset()\n",
    "        return data\n",
    "    \n",
    "    def kfold_data(self, data):\n",
    "        import sklearn\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        kf.get_n_splits(data)\n",
    "        kfold_data = {}\n",
    "        for index, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "            train_data = data[train_index]\n",
    "            test_data = data[test_index]\n",
    "            kfold_data[index] = {'train' : train_data, 'test' : test_data}\n",
    "        return kfold_data\n",
    "\n",
    "    def kfold_return_train(self, n_fold, kfold_data):\n",
    "        output_cnt = self.output_cnt\n",
    "        if n_fold not in range(10):\n",
    "            print('{} is not in range(10)'.format(n_fold))\n",
    "            raise NameError('Change n_fold')\n",
    "        train_data = kfold_data[n_fold]['train']\n",
    "        #test_data = kfold_data[n_fold]['test']\n",
    "        train_data_input = train_data[:, :-output_cnt]\n",
    "        train_data_output = train_data[:, -output_cnt:]\n",
    "        return [train_data_input, train_data_output]\n",
    "    \n",
    "    def kfold_return_test(self, n_fold, kfold_data):\n",
    "        output_cnt = self.output_cnt\n",
    "        if n_fold not in range(10):\n",
    "            print('{} is not in range(10)'.format(n_fold))\n",
    "            raise NameError('Change n_fold')\n",
    "        #train_data = kfold_data[n_fold]['train']\n",
    "        test_data = kfold_data[n_fold]['test']\n",
    "        test_data_input = test_data[:, :-output_cnt]\n",
    "        test_data_output = test_data[:, -output_cnt:]\n",
    "        return [test_data_input, test_data_output]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kfold_trainning = steal_model_optimize()\n",
    "result_dict = kfold_trainning.model_train_multiprocess()\n",
    "result =  kfold_trainning.kfold_model_evaluate_result(result_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.001)])\n",
      "\n",
      "\n",
      "acc: 0.3077, loss: 1815260640.9782\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 2.29321845649845e-05)])\n",
      "\n",
      "\n",
      "acc: 0.2917, loss: 28732818.3532\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.027086650185853486)])\n",
      "\n",
      "\n",
      "acc: 0.3256, loss: 50993490002.2719\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 6.310234413705417e-06)])\n",
      "\n",
      "\n",
      "acc: 0.2798, loss: 11121550.7399\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 2.164278482151934e-06)])\n",
      "\n",
      "\n",
      "acc: 0.3040, loss: 3254972.4486\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.2133593719123133)])\n",
      "\n",
      "\n",
      "acc: 0.3813, loss: 321961579654.8984\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.7784617614896534)])\n",
      "\n",
      "\n",
      "acc: 0.3663, loss: 1258873404725.2192\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.01932340377789457)])\n",
      "\n",
      "\n",
      "acc: 0.3256, loss: 34246919852.8727\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.0016615322247186908)])\n",
      "\n",
      "\n",
      "acc: 0.3163, loss: 2346437471.2304\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.00017772611215676338)])\n",
      "\n",
      "\n",
      "acc: 0.3535, loss: 293373100.0524\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 3.6673726216860864e-05)])\n",
      "\n",
      "\n",
      "acc: 0.3307, loss: 66291979.1853\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 1.0)])\n",
      "\n",
      "\n",
      "acc: 0.3235, loss: 2369385958658.0059\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9997507795981417)])\n",
      "\n",
      "\n",
      "acc: 0.3004, loss: 1899321657362.4714\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9990582238309693)])\n",
      "\n",
      "\n",
      "acc: 0.3163, loss: 1889384454872.0112\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9997746486091319)])\n",
      "\n",
      "\n",
      "acc: 0.3395, loss: 1284712438653.8008\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9972157802561175)])\n",
      "\n",
      "\n",
      "acc: 0.3575, loss: 1485923213907.7769\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9999639971540043)])\n",
      "\n",
      "\n",
      "acc: 0.2716, loss: 1735279536785.0276\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 1.0038916330039254e-06)])\n",
      "\n",
      "\n",
      "acc: 0.2973, loss: 1278966.0454\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 1.0004433981462736e-06)])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/skopt/optimizer/optimizer.py:409: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3045, loss: 1568735.2102\n",
      "\n",
      "\n",
      "======================end trainning=======================\n",
      "======================start trainning=======================\n",
      "dict_items([('LEARNING_RATE', 0.9994979204591179)])\n",
      "\n",
      "\n",
      "acc: 0.3231, loss: 1597246238559.9280\n",
      "\n",
      "\n",
      "======================end trainning=======================\n"
     ]
    }
   ],
   "source": [
    "hp_dict = {\n",
    "    'learning_rate' : 0.001\n",
    "}\n",
    "\n",
    "default_HP = list(hp_dict.values())\n",
    "\n",
    "def model_tunning(hp_list):\n",
    "    HP_list2dict = {\n",
    "    'LEARNING_RATE' : float(hp_list[0])\n",
    "    }\n",
    "    print(\"======================start trainning=======================\")\n",
    "    print(HP_list2dict.items())\n",
    "    print('\\n')\n",
    "    \n",
    "    tunning_model = steal_model_optimize(learning_rate = hp_list[0])\n",
    "    result_dict = tunning_model.model_train_multiprocess()\n",
    "    result = tunning_model.kfold_model_evaluate_result(result_dict, verbose=1)\n",
    "    loss = -result[0]\n",
    "    \n",
    "\n",
    "    print('\\n')\n",
    "    print(\"======================end trainning=======================\")\n",
    "    return loss\n",
    "\n",
    "#dim_RND_MEAN_nodes = Integer(low=-100, high=100, name='RND_MEAN')\n",
    "#dim_RND_STD_nodes = Real(low=1e-8, high=1.0, prior='log-uniform', name='RND_STD')\n",
    "dim_learning_rate_nodes = Real(low=1e-6, high=1.0, prior='log-uniform',name='LEARNING_RATE')\n",
    "\n",
    "dimension_HP = [\n",
    "                #dim_RND_MEAN_nodes  ,\n",
    "                #dim_RND_STD_nodes  ,\n",
    "                dim_learning_rate_nodes\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "n_cell = 20\n",
    "n_random_starts = 10\n",
    "\n",
    "gp_fitting = gp_minimize(func=model_tunning,\n",
    "                        dimensions=dimension_HP,\n",
    "                        n_calls=n_cell,\n",
    "                        n_random_starts=n_random_starts,\n",
    "                        acq_func='EI',\n",
    "                        x0=default_HP\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2133593719123133"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_lr = gp_fitting.x[0]\n",
    "opt_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3550, loss: 313555067580.7592\n"
     ]
    }
   ],
   "source": [
    "kfold_trainning = steal_model_optimize(learning_rate=opt_lr)\n",
    "result_dict = kfold_trainning.model_train_multiprocess()\n",
    "result =  kfold_trainning.kfold_model_evaluate_result(result_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3189, loss: 1675220135.4697\n"
     ]
    }
   ],
   "source": [
    "kfold_trainning = steal_model_optimize(learning_rate=0.001)\n",
    "result_dict = kfold_trainning.model_train_multiprocess()\n",
    "result =  kfold_trainning.kfold_model_evaluate_result(result_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross-entropy  \n",
    "$L(y,\\hat{y})=-{1 \\over N} \\sum_{i=0}^N (y \\log (\\hat{y}_i) + (1-y) \\log(1-\\hat{y}_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steel_model_bi:\n",
    "    def __init__(self,learning_rate = 0.001,n_fold = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_shape = 27 #input_cnt\n",
    "        self.output_shape = 7 #output_cnt\n",
    "        \n",
    "        self.model = self.define_model()\n",
    "        \n",
    "        \n",
    "    def init_data(self, train_data, test_data):\n",
    "        self.X_train = train_data[0]\n",
    "        self.Y_train = train_data[1]\n",
    "        self.X_test = test_data[0]\n",
    "        self.Y_test = test_data[1]\n",
    "        \n",
    "    def define_model(self,verbose = 0):\n",
    "        x = Input(shape=(self.input_shape))\n",
    "        y = Dense(self.output_shape, activation='softmax')(x)\n",
    "        __model = Model(x, y)\n",
    "        if verbose is not 0: __model.summary()\n",
    "        return __model\n",
    "    \n",
    "    def model_compile(self):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer,\n",
    "                           loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                           metrics=['accuracy']\n",
    "                          )\n",
    "    \n",
    "    def model_fit(self,verbose = 0):\n",
    "        self.model.fit(x = self.X_train, y = self.Y_train,\n",
    "                       #validation_split=0.05,\n",
    "                       #shuffle=True,\n",
    "                       validation_data = (self.X_test, self.Y_test),\n",
    "                       batch_size = 64,\n",
    "                       epochs = 10,\n",
    "                       verbose = verbose\n",
    "                      )\n",
    "        \n",
    "    def model_evaluate(self,verbose=0):\n",
    "        output = self.model.predict(self.X_test,verbose=verbose)\n",
    "        test_loss, test_acc = self.model.evaluate(self.X_test, self.Y_test,verbose=verbose)\n",
    "        \n",
    "        if verbose is not 0:\n",
    "            print(\"acc: {:2.4f}, loss: {:2.4f}\".format(test_acc, test_loss))\n",
    "        \n",
    "        result_dict = {'acc' : test_acc, 'loss' : test_loss}\n",
    "\n",
    "        return result_dict\n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steal_model_optimize_bi:\n",
    "    def __init__(self,learning_rate = 0.01):\n",
    "        self.input_cnt, self.output_cnt = 27, 7\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "        raw_data = self.load_steal_data()\n",
    "        self._kfold_data = self.kfold_data(raw_data)\n",
    "        \n",
    "        \n",
    "    def model_trainning(self, train_data, test_data, proc_num=None, return_dict = None):\n",
    "        model_init = steel_model_bi(learning_rate=self.learning_rate)\n",
    "        model_init.init_data(train_data = train_data,\n",
    "                             test_data = test_data)\n",
    "        model_init.model_compile()\n",
    "        model_init.model_fit(verbose=0)\n",
    "        model_result_dict = model_init.model_evaluate(verbose=0)\n",
    "        \n",
    "        if proc_num is not None and return_dict is not None:\n",
    "            return_dict[proc_num] = model_result_dict\n",
    "\n",
    "        return model_result_dict\n",
    "        \n",
    "    def model_train_multiprocess(self,):\n",
    "        kfold_data_set = []\n",
    "        for index in range(10):\n",
    "            train_data = self.kfold_return_train(n_fold=index, kfold_data=self._kfold_data)\n",
    "            test_data = self.kfold_return_test(n_fold=index, kfold_data=self._kfold_data)\n",
    "            kfold_data_set.append((train_data, test_data))\n",
    "            \n",
    "        manager = Manager()\n",
    "        return_dict = manager.dict()\n",
    "        \n",
    "        procs = []\n",
    "        for index, (train_data, test_data) in enumerate(kfold_data_set):\n",
    "            #print(train_data[0].shape, test_data[0].shape)\n",
    "            proc = Process(target=self.model_trainning,\n",
    "                           args=(train_data, test_data,\n",
    "                                 index, return_dict),\n",
    "                           name='{}-fold'.format(index)\n",
    "                          )\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "            \n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "            \n",
    "        return return_dict\n",
    "    \n",
    "    \n",
    "    def kfold_model_evaluate_result(self,return_dict, verbose = 0):\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        losses = []\n",
    "        \n",
    "        for index in range(10):\n",
    "            acc     = return_dict[index]['acc']\n",
    "            loss    = return_dict[index]['loss']\n",
    "\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        mean_acc    = np.array(accuracies).mean()\n",
    "        mean_loss = np.array(losses).mean()\n",
    "        \n",
    "        return_result_list = [mean_acc, mean_loss]\n",
    "        #print(return_result_list)\n",
    "        if verbose is not 0:\n",
    "            print(\"acc: {:2.4f}, loss: {:2.4f}\".format(*return_result_list))\n",
    "        \n",
    "\n",
    "        return return_result_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_steal_data(self,):\n",
    "        data, x, y = load_steal_dataset()\n",
    "        return data\n",
    "    \n",
    "    def kfold_data(self, data):\n",
    "        import sklearn\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        kf.get_n_splits(data)\n",
    "        kfold_data = {}\n",
    "        for index, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "            train_data = data[train_index]\n",
    "            test_data = data[test_index]\n",
    "            kfold_data[index] = {'train' : train_data, 'test' : test_data}\n",
    "        return kfold_data\n",
    "\n",
    "    def kfold_return_train(self, n_fold, kfold_data):\n",
    "        output_cnt = self.output_cnt\n",
    "        if n_fold not in range(10):\n",
    "            print('{} is not in range(10)'.format(n_fold))\n",
    "            raise NameError('Change n_fold')\n",
    "        train_data = kfold_data[n_fold]['train']\n",
    "        #test_data = kfold_data[n_fold]['test']\n",
    "        train_data_input = train_data[:, :-output_cnt]\n",
    "        train_data_output = train_data[:, -output_cnt:]\n",
    "        return [train_data_input, train_data_output]\n",
    "    \n",
    "    def kfold_return_test(self, n_fold, kfold_data):\n",
    "        output_cnt = self.output_cnt\n",
    "        if n_fold not in range(10):\n",
    "            print('{} is not in range(10)'.format(n_fold))\n",
    "            raise NameError('Change n_fold')\n",
    "        #train_data = kfold_data[n_fold]['train']\n",
    "        test_data = kfold_data[n_fold]['test']\n",
    "        test_data_input = test_data[:, :-output_cnt]\n",
    "        test_data_output = test_data[:, -output_cnt:]\n",
    "        return [test_data_input, test_data_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7631, loss: 3.6328\n"
     ]
    }
   ],
   "source": [
    "kfold_trainning = steal_model_optimize_bi(learning_rate=0.001)\n",
    "result_dict = kfold_trainning.model_train_multiprocess()\n",
    "result =  kfold_trainning.kfold_model_evaluate_result(result_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
